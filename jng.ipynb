{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "def get_soup(url):\n",
    "\n",
    "    header = {\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36Name\"\n",
    "    }\n",
    "    time.sleep(1)\n",
    "    content = requests.get(url , headers = header).text\n",
    "\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "# import re\n",
    "# def striphtml(data):\n",
    "#     clean = re.compile('<.*?>')\n",
    "#     clean = re.sub(clean, ' ', data)\n",
    "#     clean = clean.replace(u'\\xa0', u' ')\n",
    "#     return clean\n",
    "\n",
    "# def splithtml(data):\n",
    "   \n",
    "#     newclean=[]\n",
    "#     clean =  re.split(r'<.*?>', data)\n",
    "#     clean = [data.replace(u'\\xa0', u' ') for data in clean]\n",
    "#     clean2 = [x for x in clean if x.strip()]\n",
    "#     return clean2\n",
    "# option = webdriver.ChromeOptions()\n",
    "# option.add_arg\n",
    "\n",
    "\n",
    "# exec_path = \"chromedriver\"\n",
    "# browser = webdriver.Chrome(executable_path=exec_path, chrome_options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def parseResults(results, querymonth, queryyear):\n",
    "    jobpostings = []\n",
    "\n",
    "    for result in results:\n",
    "        \n",
    "        job_info = result.text.split(\"\\n\")\n",
    "        \n",
    "        date = datetime.datetime.strptime(job_info[6], \"%B %d, %Y\")\n",
    "        month = date.month\n",
    "        year = date.year\n",
    "        \n",
    "        \n",
    "        dct={}\n",
    "        #only parse info if date in query range\n",
    "        if ((month==querymonth)&(year==queryyear)):\n",
    "            \n",
    "            dct['JOB TITLE'] = job_info[0]\n",
    "            dct['LOCATION'] = job_info[2]\n",
    "            dct['FUNCTION'] = job_info[4]\n",
    "            dct['DATE'] = job_info[6]\n",
    "\n",
    "            dct['LANGUAGE'] = job_info[-1]\n",
    "            job_url = result.find_element_by_tag_name('a').get_attribute('href')\n",
    "            dct['URL'] = job_url\n",
    "            jobdetails = parseJobDescription(job_url)\n",
    "            dct.update(jobdetails)\n",
    "\n",
    "            jobpostings.append(dct)\n",
    "    \n",
    "    #return extracted jobpostings and month/year of last posting\n",
    "    return jobpostings, month, year\n",
    "\n",
    "def parseJobDescription(url):\n",
    "    time.sleep(1)\n",
    "    \n",
    "    dct = {}\n",
    "    \n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "\n",
    "        dct['job_title'] = soup.find('h1',{'class':'job-title'}).text\n",
    "        dct['location'] = soup.find('div', {'class':\"job-location job-categories\"}).text.replace(\"Locations:\",\"\").strip()\n",
    "        dct['function'] = soup.find('span', {'itemprop':\"occupationalCategory\"}).find_all('span')[-1].text\n",
    "        dct['requisition_id'] = soup.find('h5', {'class':\"job-description-reqid\"}).text.replace(\"Requisition ID:\",\"\").strip()\n",
    "        dct['job_title'] = soup.find('h1',{'class':'job-title'}).text\n",
    "\n",
    "        jdElements = soup.find('div',{'class':'jibe-job-description'}).find_all(['p','ul','li'])\n",
    "        jd = []\n",
    "\n",
    "        for element in jdElements:\n",
    "            text = element.text.split()\n",
    "            text = \" \".join([word.strip() for word in text])\n",
    "            if text:\n",
    "                jd.append(text)\n",
    "\n",
    "        dct['job_description'] = jd\n",
    "    except:\n",
    "        print(\"error reading job @ {}\".format(url))\n",
    "    return dct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdate = datetime.datetime.now()\n",
    "querymonth = currentdate.month - 1\n",
    "queryyear = currentdate.year\n",
    "\n",
    "month = querymonth\n",
    "year = queryyear\n",
    "jobpostings = []\n",
    "\n",
    "#next up - april\n",
    "i=54\n",
    "browser = webdriver.Chrome()\n",
    "#continue to get search listings if month = querymonth or later\n",
    "# browser.manage().timeouts().implicitlyWait()\n",
    "while ((month>=querymonth) & (year == queryyear)):\n",
    "    url = \"{BASEURL}\".format(i)\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        browser.get(url)\n",
    "        time.sleep(5)\n",
    "        results = browser.find_elements_by_class_name('job-result')        \n",
    "        results_parsed, month, year = parseResults(results, querymonth, queryyear)\n",
    "        \n",
    "        jobpostings.extend(results_parsed)\n",
    "        print(\"Read {}\".format(url))\n",
    "    except:\n",
    "        print(\"Error reading {}\".format(url))\n",
    "    i+=1\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobpostings)\n",
    "pprint.pprint(jobpostings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "datetoday = datetime.date.today()\n",
    "date = datetoday.strftime(\"%Y-%B-%d %H:%M\").split()[0]\n",
    "\n",
    "filename = '{name_}'+date+'.json'\n",
    "with open(filename, 'w') as fp:\n",
    "    json.dump(jobpostings, fp, indent=4)\n",
    "\n",
    "# with open(filename, 'r') as fp:\n",
    "#     readdata = json.load(fp)\n",
    "# # pprint.pprint(readdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
