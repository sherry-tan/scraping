{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "def get_soup(url):\n",
    "\n",
    "    header = {\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36Name\"\n",
    "    }\n",
    "    time.sleep(1)\n",
    "    content = requests.get(url , headers = header).text\n",
    "\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanJD(data):\n",
    "    cleaned_data =  re.split(r'<.*?>', str(data))\n",
    "    cleaned_data = [word.strip() for word in cleaned_data if word]\n",
    "    cleaned_data = [unicodedata.normalize(\"NFKD\", line) for line in cleaned_data]\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def getJobInfo(url):\n",
    "    keys = ['jobtitle', 'date_posted', 'contract_type', 'job_id', 'job_location', 'job_description']\n",
    "    dct = dict.fromkeys(keys, None)\n",
    "    \n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "        dct['jobtitle']=soup.find('h1',{'class':'h2-style'}).text\n",
    "        dct['date_posted']=soup.find('span',{'class':\"job-date job-info icon-time with-text\"}).text\n",
    "        dct['contract_type'] = soup.find('span',{'class':\"icon-contract with-text\"}).text\n",
    "        dct['job_id'] = soup.find('span',{'class':\"job-id job-info icon-job-id with-text\"}).text\n",
    "        dct['job_location'] = soup.find('span',{'class':\"job-location job-info white-text\"}).text\n",
    "        raw_jd = soup.find('div',{'class':'ats-description'})\n",
    "        dct['job_description'] = cleanJD(raw_jd)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read page 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jobpostings = []\n",
    "browser = webdriver.Chrome()\n",
    "#continue to get search listings if month = querymonth or later\n",
    "# browser.implicitly_wait(10)\n",
    "\n",
    "\n",
    "url = {URL}\n",
    "\n",
    "try:\n",
    "    browser.get(url)\n",
    "    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'search-results-list')))\n",
    "#         time.sleep(5)\n",
    "    results = browser.find_element_by_id('search-results-list').find_elements_by_tag_name('li')    \n",
    "   \n",
    "except:\n",
    "    print(\"Error reading {}\".format(url))\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "def getjobList(results):\n",
    "    jobpostings = []\n",
    "    \n",
    "    try:\n",
    "        for result in results[1:]:\n",
    "            dct={}\n",
    "            details = result.find_element_by_tag_name('a')\n",
    "            dct['url'] = details.get_attribute('href')\n",
    "            dct['ID']= details.get_attribute('data-job-id')#can remove\n",
    "            dct['job title'] = result.find_element_by_class_name('h3-style').text\n",
    "            dct['location'] = result.find_element_by_class_name('job-location').text\n",
    "            dct['date posted'] = result.find_element_by_class_name('job-date-posted').text\n",
    "            dct.update(getJobInfo(dct['url']))\n",
    "            jobpostings.append(dct)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return jobpostings\n",
    "\n",
    "#get first page results\n",
    "jobpostings.extend(getjobList(results))\n",
    "\n",
    "#get max number of pages\n",
    "max_page = browser.find_element_by_class_name('pagination-current').get_attribute('max')\n",
    "\n",
    "#parse pages 2 up to max-page\n",
    "i=2\n",
    "while i<=int(max_page):\n",
    "    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'next')))\n",
    "    next_page_btn = browser.find_element_by_class_name('next')\n",
    "    next_page_btn.click()\n",
    "#\n",
    "    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'search-results-list')))\n",
    "    results = browser.find_element_by_id('search-results-list').find_elements_by_tag_name('li')    \n",
    "    jobpostings.extend(getjobList(results))\n",
    "    print('read page {}'.format(i))\n",
    "    i+=1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
